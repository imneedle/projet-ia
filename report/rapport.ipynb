{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Projet Intelligence Artificielle\n",
            "\n",
            "Neil Desplats - Arthur Pinto Rosa - Hadrien Eyraud\n",
            "\n",
            "\n",
            "Ce rapport est également disponible au [format `ipynb`](https://raw.githubusercontent.com/imneedle/projet-ia/main/report/rapport.ipynb), sur notre [dépôt git](https://github.com/imneedle/projet-ia). Il contient le code et l'infrastructure permettant de regénérer la connaissance. Il est nécessaire d'exécuter le dernier bloc de code depuis le dépôt, car il utilise notre arborescence de scripts python (il est recommandé d'avoir Python 3.11)\n",
            "\n",
            "# Prediction de la qualité de l'air à Grenoble"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Présentation du problème\n",
            "\n",
            "La pollution de l'air représente un défi majeur dans de nombreuses villes à travers le monde, Grenoble étant un exemple illustratif. Bien que divers indicateurs soient utilisés pour évaluer la qualité de l'air, les modèles actuels ne couvrent souvent qu'un seul aspect, tel que les particules fines, les oxydes d'azote, l'ozone, ou le monoxyde de carbone. Cette étude s'appuie sur l'article suivant : https://www.sciencedirect.com/science/article/abs/pii/S1352231010003821\n",
            "\n",
            "### Pourquoi l'Intelligence Artificielle est Indispensable\n",
            "\n",
            "L'utilisation de l'intelligence artificielle se justifie par la complexité inhérente à la prédiction de la qualité de l'air, qui ne peut être pleinement appréhendée par une simple approche statistique. Voici quelques raisons majeures :\n",
            "\n",
            "1. Relations Non Linéaires\n",
            "\n",
            "Les relations entre les différents indicateurs de qualité de l'air, les conditions météorologiques et les caractéristiques urbaines sont souvent non linéaires et complexes. L'IA est capable de capturer ces relations subtiles, là où les modèles statistiques classiques pourraient échouer.\n",
            "\n",
            "2. Adaptabilité Dynamique\n",
            "\n",
            "La qualité de l'air évolue de manière dynamique en fonction de divers facteurs. L'IA peut s'adapter et apprendre des modèles évolutifs, permettant ainsi une meilleure anticipation des changements rapides, contrairement à des modèles statistiques qui pourraient manquer de flexibilité.\n",
            "\n",
            "3. Gestion de Données Hétérogènes\n",
            "\n",
            "Les données sur la qualité de l'air, les prévisions météorologiques et les caractéristiques urbaines peuvent être hétérogènes et volumineuses. L'IA excelle dans la gestion et l'analyse de ces données complexes, fournissant ainsi une vision intégrée.\n",
            "\n",
            "4. Facteurs Cachés\n",
            "\n",
            "Certains facteurs influençant la qualité de l'air peuvent être cachés ou difficilement détectables par des approches statistiques traditionnelles. L'IA peut identifier des schémas subtils et des corrélations non évidentes, améliorant ainsi la précision des prédictions.\n",
            "\n",
            "### Objectif du Projet\n",
            "\n",
            "Nous souhaitons développer une solution  basée sur l'intelligence artificielle pour prédire la qualité de l'air à court terme dans la ville de Grenoble. En combinant la puissance de l'IA avec des données météorologiques et de pollution, nous visons à créer un modèle adaptable et holistique, allant au-delà des limites des approches statistiques conventionnelles."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Jeu de données\n",
            "\n",
            "### Données Météorologiques\n",
            "\n",
            "Nous avons exploité les données de l'API OpenWeatherMap pour recueillir des informations météorologiques détaillées sur Grenoble. Ces données comprennent des paramètres tels que l'humidité, le point de rosée, la température moyenne, la vitesse et la direction du vent, etc. Cette approche nous a permis de constituer un ensemble de données riche et diversifié, couvrant une période étendue de plus d'un an et demi. La qualité de ces données météorologiques est cruciale pour établir des relations significatives entre les conditions météorologiques et la qualité de l'air.\n",
            "\n",
            "### Données de Qualité de l'Air\n",
            "\n",
            "Les données sur la qualité de l'air, essentielles à notre modèle, ont été obtenues à partir de l'API OpenMétéo. Ces données comprennent une variété de polluants atmosphériques tels que les particules fines, le monoxyde de carbone, le monoxyde d'azote, et d'autres. Nous avons récupéré des informations détaillées spécifiquement pour Grenoble sur une période similaire, soit plus d'un an et demi. Cette approche nous offre une vision complète de la pollution atmosphérique, permettant une modélisation robuste prenant en compte plusieurs polluants.\n",
            "\n",
            "### Expérience Antérieure Réussie\n",
            "\n",
            "Une expérience préliminaire menée à Louisville a montré des résultats prometteurs en utilisant les données météorologiques et les particules fines 24 heures en amont du vent. Forts de ces résultats positifs, nous étendons désormais notre approche pour inclure d'autres polluants et enrichir davantage notre ensemble de données. Cette expansion vise à renforcer la capacité de notre modèle à anticiper la qualité de l'air dans diverses conditions atmosphériques et environnementales.\n",
            "\n",
            "### Objectif d'Entraînement du Modèle\n",
            "\n",
            "L'ensemble de données ainsi constitué, couvrant une période significative et incluant une diversité de paramètres, servira à entraîner notre modèle d'intelligence artificielle. L'objectif est de permettre à notre modèle de comprendre les relations complexes entre les différentes variables météorologiques, les caractéristiques urbaines et les niveaux de pollution, afin de réaliser des prédictions précises et adaptées à Grenoble, et éventuellement, à d'autres villes. L'indice de qualité de l'air prend en compte différents composants : le dioxyde de souffre, le dioxyde d'azote, l'ozone, le monoxyde de carbone et les particules fines dont le diamètre est inférieur à 10 µm et 2.5 µm. Les valeurs sont toujours comprises entre 1 et 5. Le tableau suivant indique à quoi correspond chaque valeur.\n",
            "\n",
            "![](https://cdn.discordapp.com/attachments/1158323140666081330/1199012529184198666/image.png?ex=65c0fe6f&is=65ae896f&hm=df5031455c75523a09fc6794ecc6384114648cf858c5caa9081561f46be801ba&)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "\n",
            "## Génération des Connaissances\n",
            "\n",
            "### Utilisation d'APIs Publiques\n",
            "\n",
            "Notre approche repose sur l'utilisation de deux APIs publiques, OpenWeatherMap et OpenMétéo, pour collecter des données météorologiques et de qualité de l'air spécifiques à Grenoble. Cependant, en raison des restrictions quant au nombre d'appels quotidiens autorisés par ces APIs, nous avons dû élaborer une stratégie intelligente pour récupérer un jeu de données étendu sur la dernière année et demi.\n",
            "\n",
            "### Stratégie de Récupération de Données\n",
            "\n",
            "En raison des limitations de fréquence d'appels, nous avons mis en place un processus de récupération de données qui s'adapte aux contraintes imposées par les APIs. Plutôt que de chercher à récupérer toutes les données en une seule fois, nous avons opté pour des requêtes quotidiennes, travaillant avec la contrainte de nombre d'appels par jour.\n",
            "\n",
            "### Automatisation avec Crontab\n",
            "\n",
            "Pour implémenter notre stratégie de récupération, nous avons utilisé la planification de tâches à l'aide de la crontab. Un script de récupération de données a été configuré pour s'exécuter automatiquement à des intervalles réguliers, relançant le processus de collecte de données chaque jour. Cette approche nous a permis de rester dans les limites de requêtes autorisées tout en accumulant progressivement un jeu de données complet sur une période étendue.\n",
            "\n",
            "### Agrégation des Données\n",
            "\n",
            "À chaque exécution du script, les nouvelles données récupérées quotidiennement ont été ajoutées à notre ensemble de données principal. Cela a conduit à une accumulation graduelle et régulière de données météorologiques et de qualité de l'air sur plus de 500 jours. Cette méthode nous a permis de contourner les limitations tout en garantissant une couverture temporelle significative pour l'entraînement de notre modèle.\n",
            "\n",
            "### Avantages de l'Approche\n",
            "\n",
            "1. **Respect des Limites :** En respectant les limites d'appels quotidiens, nous avons assuré une utilisation éthique et conforme des APIs publiques.\n",
            "   \n",
            "2. **Intégration Continue :** L'automatisation du processus avec la crontab a facilité une intégration continue des nouvelles données dans notre ensemble existant, garantissant une mise à jour régulière et une couverture temporelle étendue.\n",
            "\n",
            "3. **Évolutivité :** Cette méthode évolutive nous a permis de construire un ensemble de données robuste sur une période prolongée, crucial pour l'entraînement efficace de notre modèle prédictif.\n",
            "\n",
            "### Conclusion pour récupération des connaissances générées\n",
            "\n",
            "La combinaison de l'automatisation, de la planification régulière des tâches et de la gestion intelligente des quotas d'API a été essentielle pour générer un jeu de données significatif et fiable, posant ainsi les bases d'une modélisation précise de la qualité de l'air à Grenoble.\n",
            "\n",
            "En raison de cette approche, il n'est pas possible de fournir directement un script pour récupérer les connaissances accumulées sur plus de 500 jours en raison des contraintes d'API. Cependant, nous mettons à disposition le [jeu de données résultant au format CSV](https://github.com/imneedle/projet-ia/tree/main/data/training.csv). Nous offrons également la présentation de notre [script de récupération](https://github.com/imneedle/projet-ia/blob/main/src/api/local_data.py), illustrant le processus suivi pour générer cette base de connaissances étendue."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Recherche du meilleur modèle\n",
            "\n",
            "Pour déterminer le meilleur modèle pour notre projet, nous avons effectué une revue de littérature sur divers algorithmes d'apprentissage automatique utilisés pour la prédiction de la qualité de l'air.\n",
            "\n",
            "L'un des avantages de KNN est sa simplicité et sa facilité de mise en œuvre. C'est un algorithme non paramétrique qui ne fait aucune hypothèse sur la distribution sous-jacente des données. Au lieu de cela, il s'appuie sur la proximité des points de données pour faire des prédictions. Cela rend KNN adapté pour capturer des relations complexes et non linéaires entre les indicateurs de qualité de l'air et les variables météorologiques.\n",
            "\n",
            "De plus, KNN est un algorithme polyvalent qui peut gérer à la fois les tâches de régression et de classification. Dans notre cas, nous sommes intéressés par la prédiction de l'indice de qualité de l'air (AQI), qui est une variable continue. KNN peut être facilement adapté aux tâches de régression en moyennant les valeurs des k plus proches voisins.\n",
            "\n",
            "Ces avantages ont également étés présentés par Pochai et al. (2023), qui ont publié des travaux sur la prédiction des niveaux de PM2.5 grâce à KNN pendant que nous commencions ces travaux.\n",
            "\n",
            "Sur la base de cette revue de littérature et de l'adéquation de KNN pour notre tâche de régression, nous avons décidé d'explorer ses performances dans notre projet. Cependant, il est important de noter que le choix du meilleur modèle dépend finalement des caractéristiques spécifiques de l'ensemble de données et du problème en question. Par conséquent, nous comparerons les performances de KNN avec d'autres algorithmes d'apprentissage automatique pour déterminer le meilleur modèle pour notre tâche de prédiction de la qualité de l'air."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import sys\n",
            "import os\n",
            "from datetime import datetime\n",
            "sys.path.append(os.path.join(os.path.abspath('../')))\n",
            "\n",
            "import requests\n",
            "import pandas as pd\n",
            "import time\n",
            "import numpy as np\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "\n",
            "from src.api.api import Api\n",
            "from src.api.pollution_api import PollutionApi\n",
            "from src.api.future_pollution import FuturePollutionApi\n",
            "from src.api.predicted_weather import PredictedWeatherApi\n",
            "from src.util.position_offsetter import PositionOffsetter\n",
            "\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.neighbors import KNeighborsRegressor\n",
            "from sklearn.ensemble import RandomForestRegressor\n",
            "from sklearn.ensemble import GradientBoostingRegressor\n",
            "from sklearn.ensemble import AdaBoostRegressor\n",
            "from sklearn.neural_network import MLPRegressor\n",
            "from sklearn.naive_bayes import GaussianNB\n",
            "from sklearn.linear_model import Perceptron"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "data = pd.read_csv('../data/training.csv')\n",
            "\n",
            "data.drop(columns=['time'], inplace=True)\n",
            "\n",
            "train_df, test_df = train_test_split(data, test_size=0.3)\n",
            "\n",
            "train_df = train_df.dropna()\n",
            "test_df = test_df.dropna()\n",
            "\n",
            "train_df = train_df.reset_index(drop=True)\n",
            "train_df.drop(columns=['co','no','no2','o3','so2','pm2_5','pm10','nh3'], inplace=True)\n",
            "test_df = test_df.reset_index(drop=True)\n",
            "test_df.drop(columns=['co','no','no2','o3','so2','pm2_5','pm10','nh3'], inplace=True)\n",
            "\n",
            "stats={}"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "n_neighbors = range(1, 16)\n",
            "\n",
            "maes = []\n",
            "rmses = []\n",
            "training_times = []\n",
            "prediction_times = []\n",
            "\n",
            "for k in n_neighbors:\n",
            "\n",
            "    start_time = time.time()\n",
            "\n",
            "    knn_model = KNeighborsRegressor(n_neighbors=k)\n",
            "    knn_model.fit(train_df.drop(columns=['aqi']), train_df['aqi'])\n",
            "\n",
            "    end_time = time.time()\n",
            "    training_time = end_time - start_time\n",
            "\n",
            "    start_time = time.time()    \n",
            "    predictions = knn_model.predict(test_df.drop(columns=['aqi']))\n",
            "    end_time = time.time()\n",
            "    prediction_time = end_time - start_time\n",
            "    \n",
            "    maes.append(np.mean(np.abs(predictions - test_df['aqi'])))\n",
            "    rmses.append(np.sqrt(np.mean((predictions - test_df['aqi'])**2)))\n",
            "    training_times.append(training_time)\n",
            "    prediction_times.append(prediction_time)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "plt.figure(figsize=(10, 6))\n",
            "\n",
            "plt.subplot(2, 2, 1)\n",
            "plt.plot(n_neighbors, training_times)\n",
            "plt.title('Training Time')\n",
            "plt.xlabel('Number of Neighbors')\n",
            "plt.ylabel('Time (s)')\n",
            "plt.yscale('log')\n",
            "\n",
            "plt.subplot(2, 2, 2)\n",
            "plt.plot(n_neighbors, prediction_times)\n",
            "plt.title('Prediction Time')\n",
            "plt.xlabel('Number of Neighbors')\n",
            "plt.ylabel('Time (s)')\n",
            "plt.yscale('log')\n",
            "\n",
            "plt.subplot(2, 2, 3)\n",
            "plt.plot(n_neighbors, maes)\n",
            "plt.title('Mean Absolute Error (MAE)')\n",
            "plt.xlabel('Number of Neighbors')\n",
            "plt.ylabel('Error')\n",
            "\n",
            "plt.subplot(2, 2, 4)\n",
            "plt.plot(n_neighbors, rmses)\n",
            "plt.title('Root Mean Squared Error (RMSE)')\n",
            "plt.xlabel('Number of Neighbors')\n",
            "plt.ylabel('Error')\n",
            "\n",
            "plt.tight_layout()\n",
            "plt.show()\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Comparaison avec d'autres modèles\n",
            "\n",
            "Nous avons constaté que le meilleur choix pour le nombre de voisins est aux alentours de 4 ou 5. Nous avons donc pris la décision d'utiliser 4 voisins.\n",
            "\n",
            "Afin de vérifier que ce modèle permet une bonne précision, nous comparerons KNN à d'autres modèles qui nous semblaient appropriés pour cette tâche : \n",
            "- Random Forest\n",
            "- Gradient Boosting\n",
            "- AdaBoost\n",
            "- Multi-layer Perceptron\n",
            "\n",
            "Nous avons aussi séléctionné deux modèles qui nous semblaient moins adapté, pour vérifier si les autres algorithmes étaient effectivement meilleurs :\n",
            "- Gaussian Naive Bayes\n",
            "- Perceptron\n",
            "\n",
            "Les scripts suivants montrent nos efforts de recherche.\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "start_time = time.time()\n",
            "\n",
            "knn_model = KNeighborsRegressor(n_neighbors=4)\n",
            "knn_model.fit(train_df.drop(columns=['aqi']), train_df['aqi'])\n",
            "\n",
            "end_time = time.time()\n",
            "\n",
            "training_time = end_time - start_time\n",
            "\n",
            "start_time = time.time()\n",
            "\n",
            "predictions = knn_model.predict(test_df.drop(columns=['aqi']))\n",
            "\n",
            "end_time = time.time()\n",
            "\n",
            "prediction_time = end_time - start_time\n",
            "\n",
            "model = \"KNeighborsRegressor\"\n",
            "stats[model] = {\"training_time\": training_time, \"prediction_time\": prediction_time}"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "print(f\"{model = }\")\n",
            "print(f\"{training_time = }\")\n",
            "print(f\"{prediction_time = }\")\n",
            "\n",
            "# Calculate the mean absolute error (MAE)\n",
            "mae = np.mean(np.abs(predictions - test_df['aqi']))\n",
            "stats[model]['mae'] = mae\n",
            "print(\"Mean Absolute Error (MAE):\", mae)\n",
            "\n",
            "# Calculate the root mean squared error (RMSE)\n",
            "rmse = np.sqrt(np.mean((predictions - test_df['aqi'])**2))\n",
            "stats[model]['rmse'] = rmse\n",
            "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
            "\n",
            "# Calculate the coefficient of determination (R-squared)\n",
            "ss_total = np.sum((test_df['aqi'] - np.mean(test_df['aqi']))**2)\n",
            "ss_residual = np.sum((test_df['aqi'] - predictions)**2)\n",
            "r_squared = 1 - (ss_residual / ss_total)\n",
            "stats[model]['r_squared'] = r_squared\n",
            "print(\"Coefficient of Determination (R-squared):\", r_squared)\n",
            "\n",
            "# Calculate the mean absolute percentage error (MAPE)\n",
            "mape = np.mean(np.abs((predictions - test_df['aqi']) / test_df['aqi'])) * 100\n",
            "stats[model]['mape'] = mape\n",
            "print(\"Mean Absolute Percentage Error (MAPE):\", mape)\n",
            "\n",
            "\n",
            "random_predictions = test_df.sample(15)\n",
            "random_predictions['Expected Value'] = random_predictions['aqi']\n",
            "random_predictions['Predicted Value'] = predictions[random_predictions.index]\n",
            "print(random_predictions[['Expected Value', 'Predicted Value']])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "start_time = time.time()\n",
            "\n",
            "rf_model = RandomForestRegressor(n_estimators=100)\n",
            "rf_model.fit(train_df.drop(columns=['aqi']), train_df['aqi'])\n",
            "\n",
            "end_time = time.time()\n",
            "training_time = end_time - start_time\n",
            "\n",
            "start_time = time.time()\n",
            "predictions = rf_model.predict(test_df.drop(columns=['aqi']))\n",
            "end_time = time.time()\n",
            "prediction_time = end_time - start_time\n",
            "\n",
            "model = \"RandomForestRegressor\"\n",
            "stats[model] = {\"training_time\": training_time, \"prediction_time\": prediction_time}"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "print(f\"{model = }\")\n",
            "print(f\"{training_time = }\")\n",
            "print(f\"{prediction_time = }\")\n",
            "\n",
            "# Calculate the mean absolute error (MAE)\n",
            "mae = np.mean(np.abs(predictions - test_df['aqi']))\n",
            "stats[model]['mae'] = mae\n",
            "print(\"Mean Absolute Error (MAE):\", mae)\n",
            "\n",
            "# Calculate the root mean squared error (RMSE)\n",
            "rmse = np.sqrt(np.mean((predictions - test_df['aqi'])**2))\n",
            "stats[model]['rmse'] = rmse\n",
            "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
            "\n",
            "# Calculate the coefficient of determination (R-squared)\n",
            "ss_total = np.sum((test_df['aqi'] - np.mean(test_df['aqi']))**2)\n",
            "ss_residual = np.sum((test_df['aqi'] - predictions)**2)\n",
            "r_squared = 1 - (ss_residual / ss_total)\n",
            "stats[model]['r_squared'] = r_squared\n",
            "print(\"Coefficient of Determination (R-squared):\", r_squared)\n",
            "\n",
            "# Calculate the mean absolute percentage error (MAPE)\n",
            "mape = np.mean(np.abs((predictions - test_df['aqi']) / test_df['aqi'])) * 100\n",
            "stats[model]['mape'] = mape\n",
            "print(\"Mean Absolute Percentage Error (MAPE):\", mape)\n",
            "\n",
            "\n",
            "random_predictions = test_df.sample(15)\n",
            "random_predictions['Expected Value'] = random_predictions['aqi']\n",
            "random_predictions['Predicted Value'] = predictions[random_predictions.index]\n",
            "print(random_predictions[['Expected Value', 'Predicted Value']])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "start_time = time.time()\n",
            "\n",
            "gb_model = GradientBoostingRegressor()\n",
            "gb_model.fit(train_df.drop(columns=['aqi']), train_df['aqi'])\n",
            "\n",
            "end_time = time.time()\n",
            "training_time = end_time - start_time\n",
            "\n",
            "start_time = time.time()\n",
            "\n",
            "predictions = gb_model.predict(test_df.drop(columns=['aqi']))\n",
            "end_time = time.time()\n",
            "prediction_time = end_time - start_time\n",
            "\n",
            "model = \"GradientBoostingRegressor\"\n",
            "stats[model] = {\"training_time\": training_time, \"prediction_time\": prediction_time}"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "print(f\"{model = }\")\n",
            "print(f\"{training_time = }\")\n",
            "print(f\"{prediction_time = }\")\n",
            "\n",
            "# Calculate the mean absolute error (MAE)\n",
            "mae = np.mean(np.abs(predictions - test_df['aqi']))\n",
            "stats[model]['mae'] = mae\n",
            "print(\"Mean Absolute Error (MAE):\", mae)\n",
            "\n",
            "# Calculate the root mean squared error (RMSE)\n",
            "rmse = np.sqrt(np.mean((predictions - test_df['aqi'])**2))\n",
            "stats[model]['rmse'] = rmse\n",
            "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
            "\n",
            "# Calculate the coefficient of determination (R-squared)\n",
            "ss_total = np.sum((test_df['aqi'] - np.mean(test_df['aqi']))**2)\n",
            "ss_residual = np.sum((test_df['aqi'] - predictions)**2)\n",
            "r_squared = 1 - (ss_residual / ss_total)\n",
            "stats[model]['r_squared'] = r_squared\n",
            "print(\"Coefficient of Determination (R-squared):\", r_squared)\n",
            "\n",
            "# Calculate the mean absolute percentage error (MAPE)\n",
            "mape = np.mean(np.abs((predictions - test_df['aqi']) / test_df['aqi'])) * 100\n",
            "stats[model]['mape'] = mape\n",
            "print(\"Mean Absolute Percentage Error (MAPE):\", mape)\n",
            "\n",
            "\n",
            "random_predictions = test_df.sample(15)\n",
            "random_predictions['Expected Value'] = random_predictions['aqi']\n",
            "random_predictions['Predicted Value'] = predictions[random_predictions.index]\n",
            "print(random_predictions[['Expected Value', 'Predicted Value']])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "start_time = time.time()\n",
            "\n",
            "ada_model = AdaBoostRegressor()\n",
            "ada_model.fit(train_df.drop(columns=['aqi']), train_df['aqi'])\n",
            "\n",
            "end_time = time.time()\n",
            "training_time = end_time - start_time\n",
            "\n",
            "start_time = time.time()\n",
            "\n",
            "predictions = ada_model.predict(test_df.drop(columns=['aqi']))\n",
            "\n",
            "end_time = time.time()\n",
            "prediction_time = end_time - start_time\n",
            "\n",
            "model = \"AdaBoostRegressor\"\n",
            "stats[model] = {\"training_time\": training_time, \"prediction_time\": prediction_time}"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "print(f\"{model = }\")\n",
            "print(f\"{training_time = }\")\n",
            "print(f\"{prediction_time = }\")\n",
            "\n",
            "# Calculate the mean absolute error (MAE)\n",
            "mae = np.mean(np.abs(predictions - test_df['aqi']))\n",
            "stats[model]['mae'] = mae\n",
            "print(\"Mean Absolute Error (MAE):\", mae)\n",
            "\n",
            "# Calculate the root mean squared error (RMSE)\n",
            "rmse = np.sqrt(np.mean((predictions - test_df['aqi'])**2))\n",
            "stats[model]['rmse'] = rmse\n",
            "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
            "\n",
            "# Calculate the coefficient of determination (R-squared)\n",
            "ss_total = np.sum((test_df['aqi'] - np.mean(test_df['aqi']))**2)\n",
            "ss_residual = np.sum((test_df['aqi'] - predictions)**2)\n",
            "r_squared = 1 - (ss_residual / ss_total)\n",
            "stats[model]['r_squared'] = r_squared\n",
            "print(\"Coefficient of Determination (R-squared):\", r_squared)\n",
            "\n",
            "# Calculate the mean absolute percentage error (MAPE)\n",
            "mape = np.mean(np.abs((predictions - test_df['aqi']) / test_df['aqi'])) * 100\n",
            "stats[model]['mape'] = mape\n",
            "print(\"Mean Absolute Percentage Error (MAPE):\", mape)\n",
            "\n",
            "\n",
            "random_predictions = test_df.sample(15)\n",
            "random_predictions['Expected Value'] = random_predictions['aqi']\n",
            "random_predictions['Predicted Value'] = predictions[random_predictions.index]\n",
            "print(random_predictions[['Expected Value', 'Predicted Value']])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "start_time = time.time()\n",
            "\n",
            "mlp_model = MLPRegressor(hidden_layer_sizes=(100, 50), activation='relu', solver='adam', random_state=42)\n",
            "mlp_model.fit(train_df.drop(columns=['aqi']), train_df['aqi'])\n",
            "\n",
            "end_time = time.time()\n",
            "training_time = end_time - start_time\n",
            "\n",
            "start_time = time.time()\n",
            "\n",
            "predictions = mlp_model.predict(test_df.drop(columns=['aqi']))\n",
            "\n",
            "end_time = time.time()\n",
            "prediction_time = end_time - start_time\n",
            "\n",
            "model = \"MLPRegressor\"\n",
            "stats[model] = {\"training_time\": training_time, \"prediction_time\": prediction_time}"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "print(f\"{model = }\")\n",
            "print(f\"{training_time = }\")\n",
            "print(f\"{prediction_time = }\")\n",
            "\n",
            "# Calculate the mean absolute error (MAE)\n",
            "mae = np.mean(np.abs(predictions - test_df['aqi']))\n",
            "stats[model]['mae'] = mae\n",
            "print(\"Mean Absolute Error (MAE):\", mae)\n",
            "\n",
            "# Calculate the root mean squared error (RMSE)\n",
            "rmse = np.sqrt(np.mean((predictions - test_df['aqi'])**2))\n",
            "stats[model]['rmse'] = rmse\n",
            "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
            "\n",
            "# Calculate the coefficient of determination (R-squared)\n",
            "ss_total = np.sum((test_df['aqi'] - np.mean(test_df['aqi']))**2)\n",
            "ss_residual = np.sum((test_df['aqi'] - predictions)**2)\n",
            "r_squared = 1 - (ss_residual / ss_total)\n",
            "stats[model]['r_squared'] = r_squared\n",
            "print(\"Coefficient of Determination (R-squared):\", r_squared)\n",
            "\n",
            "# Calculate the mean absolute percentage error (MAPE)\n",
            "mape = np.mean(np.abs((predictions - test_df['aqi']) / test_df['aqi'])) * 100\n",
            "stats[model]['mape'] = mape\n",
            "print(\"Mean Absolute Percentage Error (MAPE):\", mape)\n",
            "\n",
            "\n",
            "random_predictions = test_df.sample(15)\n",
            "random_predictions['Expected Value'] = random_predictions['aqi']\n",
            "random_predictions['Predicted Value'] = predictions[random_predictions.index]\n",
            "print(random_predictions[['Expected Value', 'Predicted Value']])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "start_time = time.time()\n",
            "\n",
            "nb_model = GaussianNB()\n",
            "nb_model.fit(train_df.drop(columns=['aqi']), train_df['aqi'])\n",
            "\n",
            "end_time = time.time()\n",
            "training_time = end_time - start_time\n",
            "\n",
            "start_time = time.time()\n",
            "\n",
            "predictions = nb_model.predict(test_df.drop(columns=['aqi']))\n",
            "\n",
            "end_time = time.time()\n",
            "prediction_time = end_time - start_time\n",
            "\n",
            "model = \"GaussianNB\"\n",
            "stats[model] = {\"training_time\": training_time, \"prediction_time\": prediction_time}"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "print(f\"{model = }\")\n",
            "print(f\"{training_time = }\")\n",
            "print(f\"{prediction_time = }\")\n",
            "\n",
            "# Calculate the mean absolute error (MAE)\n",
            "mae = np.mean(np.abs(predictions - test_df['aqi']))\n",
            "stats[model]['mae'] = mae\n",
            "print(\"Mean Absolute Error (MAE):\", mae)\n",
            "\n",
            "# Calculate the root mean squared error (RMSE)\n",
            "rmse = np.sqrt(np.mean((predictions - test_df['aqi'])**2))\n",
            "stats[model]['rmse'] = rmse\n",
            "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
            "\n",
            "# Calculate the coefficient of determination (R-squared)\n",
            "ss_total = np.sum((test_df['aqi'] - np.mean(test_df['aqi']))**2)\n",
            "ss_residual = np.sum((test_df['aqi'] - predictions)**2)\n",
            "r_squared = 1 - (ss_residual / ss_total)\n",
            "stats[model]['r_squared'] = r_squared\n",
            "print(\"Coefficient of Determination (R-squared):\", r_squared)\n",
            "\n",
            "# Calculate the mean absolute percentage error (MAPE)\n",
            "mape = np.mean(np.abs((predictions - test_df['aqi']) / test_df['aqi'])) * 100\n",
            "stats[model]['mape'] = mape\n",
            "print(\"Mean Absolute Percentage Error (MAPE):\", mape)\n",
            "\n",
            "\n",
            "random_predictions = test_df.sample(15)\n",
            "random_predictions['Expected Value'] = random_predictions['aqi']\n",
            "random_predictions['Predicted Value'] = predictions[random_predictions.index]\n",
            "print(random_predictions[['Expected Value', 'Predicted Value']])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "start_time = time.time()\n",
            "\n",
            "perceptron_model = Perceptron()\n",
            "perceptron_model.fit(train_df.drop(columns=['aqi']), train_df['aqi'])\n",
            "\n",
            "end_time = time.time()\n",
            "training_time = end_time - start_time\n",
            "\n",
            "start_time = time.time()\n",
            "\n",
            "predictions = perceptron_model.predict(test_df.drop(columns=['aqi']))\n",
            "\n",
            "end_time = time.time()\n",
            "prediction_time = end_time - start_time\n",
            "\n",
            "model = \"Perceptron\"\n",
            "stats[model] = {\"training_time\": training_time, \"prediction_time\": prediction_time}"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "print(f\"{model = }\")\n",
            "print(f\"{training_time = }\")\n",
            "print(f\"{prediction_time = }\")\n",
            "\n",
            "# Calculate the mean absolute error (MAE)\n",
            "mae = np.mean(np.abs(predictions - test_df['aqi']))\n",
            "stats[model]['mae'] = mae\n",
            "print(\"Mean Absolute Error (MAE):\", mae)\n",
            "\n",
            "# Calculate the root mean squared error (RMSE)\n",
            "rmse = np.sqrt(np.mean((predictions - test_df['aqi'])**2))\n",
            "stats[model]['rmse'] = rmse\n",
            "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
            "\n",
            "# Calculate the coefficient of determination (R-squared)\n",
            "ss_total = np.sum((test_df['aqi'] - np.mean(test_df['aqi']))**2)\n",
            "ss_residual = np.sum((test_df['aqi'] - predictions)**2)\n",
            "r_squared = 1 - (ss_residual / ss_total)\n",
            "stats[model]['r_squared'] = r_squared\n",
            "print(\"Coefficient of Determination (R-squared):\", r_squared)\n",
            "\n",
            "# Calculate the mean absolute percentage error (MAPE)\n",
            "mape = np.mean(np.abs((predictions - test_df['aqi']) / test_df['aqi'])) * 100\n",
            "stats[model]['mape'] = mape\n",
            "print(\"Mean Absolute Percentage Error (MAPE):\", mape)\n",
            "\n",
            "\n",
            "random_predictions = test_df.sample(15)\n",
            "random_predictions['Expected Value'] = random_predictions['aqi']\n",
            "random_predictions['Predicted Value'] = predictions[random_predictions.index]\n",
            "print(random_predictions[['Expected Value', 'Predicted Value']])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "models = list(stats.keys())\n",
            "training_times = [stats[model]['training_time'] for model in models]\n",
            "prediction_times = [stats[model]['prediction_time'] for model in models]\n",
            "maes = [stats[model]['mae'] for model in models]\n",
            "rmses = [stats[model]['rmse'] for model in models]\n",
            "r_squareds = [stats[model]['r_squared'] for model in models]\n",
            "mapes = [stats[model]['mape'] for model in models]\n",
            "\n",
            "plt.figure(figsize=(10, 10))\n",
            "\n",
            "plt.subplot(2, 2, 1)\n",
            "plt.bar(models, training_times)\n",
            "plt.title('Training Time')\n",
            "plt.xlabel('Model')\n",
            "plt.ylabel('Time (s)')\n",
            "plt.yscale('log')\n",
            "plt.xticks(rotation=45, ha='right', rotation_mode='anchor')\n",
            "\n",
            "plt.subplot(2, 2, 2)\n",
            "plt.bar(models, prediction_times)\n",
            "plt.title('Prediction Time')\n",
            "plt.xlabel('Model')\n",
            "plt.ylabel('Time (s)')\n",
            "plt.yscale('log')\n",
            "plt.xticks(rotation=45, ha='right', rotation_mode='anchor')\n",
            "\n",
            "plt.subplot(2, 2, 3)\n",
            "plt.bar(models, maes)\n",
            "plt.title('Mean Absolute Error (MAE)')\n",
            "plt.xlabel('Model')\n",
            "plt.ylabel('Error')\n",
            "plt.xticks(rotation=45, ha='right', rotation_mode='anchor')\n",
            "\n",
            "plt.subplot(2, 2, 4)\n",
            "plt.bar(models, rmses)\n",
            "plt.title('Root Mean Squared Error (RMSE)')\n",
            "plt.xlabel('Model')\n",
            "plt.ylabel('Error')\n",
            "plt.xticks(rotation=45, ha='right', rotation_mode='anchor')\n",
            "\n",
            "plt.tight_layout()\n",
            "plt.show()\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Discussion sur la qualité des modèles évalués\n",
            "\n",
            "Nous constatons que les modèles que nous avons choisis pour leur qualité sont effectivement meilleurs que les autres. KNN fournit des résultats plutôt précis en comparaison aux autres modèles, mais ce n'est pas le meilleur. Random Forest et Gradient Boosting sont légèrement meilleurs.\n",
            "\n",
            "L'erreur est acceptable pour donner une indication de la pollution de l'air, mais n'est pas assez précise pour des domaines ou cela serait important."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Évaluation des aspects environnementaux et sociétaux\n",
            "\n",
            "Nous avons réalisé des mesures significatives de l'impact environnemental de notre projet d'intelligence artificielle en utilisant la bibliothèque Python CarbonTracker. Ces mesures ont été effectuées tant pendant la phase d'entraînement du modèle que lors de la phase de prédiction. Les résultats, présentés dans le tableau ci-dessous, sont exprimés en termes d'équivalent en mètres parcourus en voiture, une unité couramment utilisée pour quantifier les émissions de carbone. Les autres chiffres indiqués correspondent aux grammes de CO2 produits lors du fonctionnement. Le calcul de l'équivalent CO2 utilise la valeur de 58.48 gCO2/kWh, qui correspond à l'intensité moyenne à Grenoble en 2021. On admet que le modèle sera beaucoup moins entraîné qu'il ne fera de prédictions (d'où le facteur 1000 entre les 2). La machine de test est dotée d'un processeur AMD Ryzen 7 à 8 cœurs.\n",
            "\n",
            "|    Modèle              | Entrainement (x10) | Equivalent en mètres parcourus en voiture | Prédiction (x 10000) | Equivalent en mètres parcourus en voiture |\n",
            "|:------------------------:|:--------------------:|:-------------------------------------------:|:----------------------:|:-------------------------------------------:|\n",
            "| KNN                    | ~0                 | ~0                                        | 0.442                | 4.11                                      |\n",
            "| Random Forest          | 0.0411             | 0.383                                     | 0.222                | 2.07                                      |\n",
            "| Gradient Boost         | 0.0466             | 0.434                                     | 0.100                | 0.930                                     |\n",
            "| Ada Boost              | 0.00978            | 0.0910                                    | 0.173                | 1.61                                      |\n",
            "| Multi-layer Perceptron | 0.0402             | 0.374                                     | 0.325                | 3.02                                      |\n",
            "| Gaussian               | 0.000135           | 0.00125                                   | 0.101                | 0.942                                     |\n",
            "| Perceptron             | 0.000684           | 0.00637                                   | ~0                   | ~0                                        |\n",
            "\n",
            "### Analyse de l'Impact Environnemental\n",
            "\n",
            "1. Entraînement du Modèle\n",
            "\n",
            "Les modèles KNN, Gaussian et Perceptron présentent des impacts environnementaux minimes lors de la phase d'entraînement, s'approchant de zéro équivalent en mètres parcourus en voiture.\n",
            "Les modèles Random Forest, Gradient Boost, et Multi-layer Perceptron présentent des impacts plus significatifs. \n",
            "\n",
            "2. Prédiction avec le Modèle\n",
            "\n",
            "Le modèle KNN affiche un impact de prédiction plus élevé par rapport à son entraînement, avec un équivalent en mètres parcourus en voiture de 4.11.\n",
            "Le modèle Perceptron montre la plus grande efficacité environnementale lors de la phase de prédiction, avec un impact minimal approximé à 0.\n",
            "\n",
            "3. Choix du Modèle en Termes d'Impact Environnemental\n",
            "\n",
            "En considérant l'ensemble des modèles, le Perceptron se distingue par son impact environnemental relativement bas, tant pendant l'entraînement que la prédiction.\n",
            "Cependant, il est essentiel de noter que le choix du modèle optimal ne doit pas se faire uniquement sur la base de l'impact environnemental, mais également en prenant en compte la précision et la performance du modèle dans la résolution du problème spécifique de qualité de l'air. Ainsi, il est recommandé de trouver un équilibre entre l'efficacité environnementale et les performances du modèle. En tenant compte de ces 2 critères, le perceptron, qui à des prédictions trop biaisées par rapport à la réalité ne peut-être sélectionné.\n",
            "\n",
            "### Impact Sociétal du Projet\n",
            "\n",
            "Notre projet d'intelligence artificielle visant à prédire la qualité de l'air présente également des implications sociétales qui méritent d'être évaluées. Voici une analyse de l'impact potentiel de notre projet sur la société :\n",
            "\n",
            "1. Amélioration de la Santé Publique\n",
            "\n",
            "En anticipant la qualité de l'air, notre modèle offre la possibilité aux autorités locales et aux résidents de prendre des mesures préventives en cas de niveaux de pollution élevés. Cela peut contribuer à réduire les risques sanitaires associés à la pollution de l'air, améliorant ainsi la santé globale de la population.\n",
            "\n",
            "2. Urbanisme Durable\n",
            "\n",
            "Les données météorologiques et de qualité de l'air collectées peuvent être utilisées pour orienter les décisions d'urbanisme vers des solutions plus durables et respectueuses de l'environnement. Cela pourrait avoir un impact positif sur la planification urbaine en favorisant des politiques et des infrastructures qui réduisent la pollution atmosphérique.\n",
            "\n",
            "3. Sensibilisation à la Pollution de l'Air\n",
            "\n",
            "Notre projet peut contribuer à sensibiliser la population à la problématique de la pollution de l'air en fournissant des informations prédictives accessibles. Une meilleure compréhension des enjeux environnementaux peut conduire à des comportements individuels plus responsables et à des initiatives collectives en faveur de la qualité de l'air.\n",
            "\n",
            "4. Prise de Décision Informée\n",
            "\n",
            "Les données générées par notre modèle peuvent être utilisées par les autorités locales pour prendre des décisions informées en matière de gestion de la qualité de l'air. Cela inclut la mise en œuvre de mesures d'urgence, la planification de la circulation, et d'autres initiatives visant à réduire l'impact de la pollution atmosphérique.\n",
            "\n",
            "5. Accessibilité aux Données Environnementales\n",
            "\n",
            "En fournissant des données prédictives sur la qualité de l'air, notre projet contribue à rendre les informations environnementales plus accessibles au grand public. Cela renforce la transparence et l'engagement de la communauté dans les questions liées à l'environnement."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Prédiction de la qualité de l'air selon les connaissances créées\n",
            "\n",
            "Il nous est désormais possible de prédire la qualité de l'air grâce à nos modèles. Comme expliqué précédemment, nous utiliserons le modèle de Gradient Boosting. \n",
            "\n",
            "Les données de météorologie sont récupérées par les APIs mentionnées au début. Il est évidemment impossible de comparer les données prédites et réelles, mais nous nous baserons quand même sur les prédictions de OpenWeatherMap comme référence. Nous constatons que les prédictions ne sont pas parfaites, mais contrairement à celles d'OpenWeatherMap, elles ne nécéssitent pas de faire une simulation à grande échelle des polluants."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from datetime import datetime, timedelta\n",
            "from tracemalloc import start\n",
            "# Set position to Grenoble\n",
            "lat, lon = 45.185992, 5.734384\n",
            "start_date = datetime.now().replace(minute=0, second=0, microsecond=0) + timedelta(hours=1)\n",
            "end_date = datetime.now().replace(minute=0, second=0, microsecond=0) + timedelta(hours=25)\n",
            "\n",
            "appid = \"3f4dd805354d2b0a8aaf79250d2b44fe\"\n",
            "\n",
            "pollution_api = PollutionApi()\n",
            "future_pollution_api = FuturePollutionApi()\n",
            "weather_api = PredictedWeatherApi()\n",
            "\n",
            "# Get weather data at position\n",
            "weather_df = weather_api.get_dataframe({\n",
            "    \"latitude\": lat, \"longitude\": lon,\n",
            "    \"latitude\": lat, \"longitude\": lon,\n",
            "    \"start_date\": start_date.strftime('%Y-%m-%d'),\n",
            "    \"end_date\": end_date.strftime('%Y-%m-%d'),\n",
            "    \"hourly\": [\"temperature_2m\", \"relative_humidity_2m\", \"precipitation\", \"dewpoint_2m\", \"wind_speed_10m\", \"wind_direction_10m\"],\n",
            "})\n",
            "\n",
            "# Get pollution data for offsetted wind position 24h before (now)\n",
            "wind_df = weather_api.get_dataframe({\n",
            "    \"latitude\": lat, \"longitude\": lon,\n",
            "    \"start_date\": start_date.strftime('%Y-%m-%d'),\n",
            "    \"end_date\": end_date.strftime('%Y-%m-%d'),\n",
            "    \"hourly\": [\"wind_speed_100m\", \"wind_direction_100m\"],\n",
            "})\n",
            "wind_speed = wind_df['wind_speed_100m'][0]\n",
            "wind_direction = wind_df['wind_direction_100m'][0]\n",
            "\n",
            "offsetter = PositionOffsetter(lat, lon)\n",
            "offsetter.offset(wind_speed, wind_direction, .5)\n",
            "offsetted_lat, offsetted_lon = offsetter.lat, offsetter.lon\n",
            "\n",
            "offsetted_df = pollution_api.get_dataframe({\n",
            "    \"lat\": offsetted_lat, \"lon\": offsetted_lon,\n",
            "    \"start\": int((start_date - timedelta(hours=23)).timestamp()), \"end\": int((end_date - timedelta(hours=24)).timestamp()),\n",
            "    \"appid\": appid\n",
            "})\n",
            "offsetted_df = offsetted_df.rename(columns=lambda a: f'{a}_offset')\n",
            "\n",
            "offsetted_df.index += timedelta(hours=24)\n",
            "\n",
            "# Combine the data at position with the data at offsetted position\n",
            "combined_df = pd.concat([weather_df, offsetted_df], axis=1)\n",
            "combined_df = combined_df.dropna()\n",
            "\n",
            "future_y = gb_model.predict(combined_df)\n",
            "future_df = pd.DataFrame(future_y, columns=['aqi'])\n",
            "\n",
            "predicted_df = future_pollution_api.get_dataframe({\n",
            "    \"lat\": lat, \"lon\": lon,\n",
            "    \"appid\": appid\n",
            "})\n",
            "\n",
            "print(\"timestamp\", \"our prediction\", \"reference prediction\", sep=\"\\t\")\n",
            "for i in range(24):\n",
            "    print(predicted_df.index[i+1], round(future_y[i]), predicted_df['aqi'][i+1], sep=\"\\t\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Conclusion\n",
            "\n",
            "L'utilisation de l'IA pour la prédiction de la qualité de l'air semble raisonnablement adapté aux usages où la précision n'est pas critique. Cependant, cette affirmation est à restreindre à ces cas. Pour des utilisations plus importantes, nous pensons que l'utilisation des modèles météorologiques actuels est à privilégier. Il faut bien sur prendre en compte l'utilisation énergétique plus importante de ces modèles par rapport à nos expérimentations.\n",
            "\n",
            "Des recherches supplémentaires permettraient d'améliorer ce défrichage. Par exemple, il pourrait être intéressant de prendre en compte l'instant des points à prédire : l'heure de la journée, le jour de la semaine ou même le mois. La prédiction indépendante des facteurs pris en compte par l'index de qualité de l'air pourrait aussi amener à de meilleurs résultats. Cela permettrait également de calculer des index différents sans réentrainer les modèles. Cela pourrait toutefois amener à une plus importante consommation énergétique.\n",
            "\n",
            "Il pourrait être envisageable d'entrainer ces modèles sur plus de données. De plus, la présence importante des années 2020 et 2021 (particulières à l'égard de la pollution de par la pandémie de CoViD-19), limite imposée par notre source, peut être cause de biais dans les données. Nonobstant, il faut aussi prendre en compte que la prise en compte des polluants est chaque année de plus en plus importante, et donc qu'il pourrait être néfaste de récupérer des données trop anciennes.\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Bibliographie\n",
            "\n",
            "Pochai et al. *A PM2.5 Forewarning Algorithm Using k-Nearest Neighbors\n",
            "Machine Learning at Changpuek, Chiang Mai, Thailand* (2023) (https://dl.acm.org/doi/pdf/10.1145/3625704.3625749)\n",
            "\n",
            "Szymon Hoffman et al. *Air Quality Modeling with the Use of Regression Neural Networks* (2022) (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9779138/)\n",
            "\n",
            "W. Geoffrey Cobourn *An enhanced PM2.5 air quality forecast model based on nonlinear regression and back-trajectory concentrations* (2010) (https://www.sciencedirect.com/science/article/abs/pii/S1352231010003821)\n",
            "\n",
            "Modèles d'apprentissage supervisé de sci-kit https://scikit-learn.org/stable/supervised_learning.html\n",
            "\n",
            "Calculateur de consommation d'un programme https://github.com/lfwa/carbontracker\n",
            "\n",
            "Open Météo API Documentation (weather API) https://open-meteo.com/en/docs/ \n",
            "\n",
            "Open Weather Map API Documentation (air pollution API)https://openweathermap.org/api/air-pollution"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.11.6"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
